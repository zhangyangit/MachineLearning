# 决策树
在已知各种情况发生概率的基础上，通过构成决策树来求取净现值的期望值大于等于零的概率，评价项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法。
由于这种决策分支画成图形很像一棵树的枝干，故称决策树。
在机器学习中，决策树是一个预测模型，他代表的是对象属性与对象值之间的一种映射关系。
Entropy = 系统的凌乱程度，使用算法ID3, C4.5和C5.0生成树算法使用熵。
### 知识点
* 1 决策树生成算法
* 2 决策树剪枝算法
* 3 决策树可视化

## 数据信息
数据的评价标准是信息量，但是信息量的度量方法使反向定义：若一种划分能使数据的不确定性减少的越多，就意味着这种划分获取越多信息。
其度量的方式：信息熵，基尼系数。

* 信息熵
  > 计算公式：H(y) = -plogp - (1-p)log(1-p)
* 基尼系数
  > 计算公式：Gini(y) = 1-p^2(y∈Y1) - p^2(y∈Y2)

## 信息增益


## 决策树算法

### ID3算法
*  算法思想
   >ID3算法是决策树的一种，它是基于奥卡姆剃刀原理的，即用尽量用较少的东西做更多的事。
ID3算法，即Iterative Dichotomiser 3，迭代二叉树3代，是Ross Quinlan发明的一种决策树算法。
这个算法的基础就是上面提到的奥卡姆剃刀原理，越是小型的决策树越优于大的决策树，尽管如此，也不总是生成最小的树型结构，而是一个启发式算法。
在信息论中，期望信息越小，那么信息增益就越大，从而纯度就越高。
ID3算法的核心思想就是以信息增益来度量属性的选择，选择分裂后信息增益最大的属性进行分裂。
该算法采用自顶向下的贪婪搜索遍历可能的决策空间。

* 算法计算
    >计算公式：
### C4.5算法
*  算法思想
   >C4.5算法是用于生成决策树的一种经典算法，是ID3算法的一种延伸和优化。C4.5算法对ID3算法主要做了一下几点改进：
   + 1 通过信息增益率选择分裂属性，克服了ID3算法中通过信息增益倾向于选择拥有多个属性值的属性作为分裂属性的不足；
   + 2 能够处理离散型和连续型的属性类型，即将连续型的属性进行离散化处理；
   + 3 构造决策树之后进行剪枝操作；
   + 4 能够处理具有缺失属性值的训练数据。

   >C4.5算法训练的结果是一个分类模型，这个分类模型可以理解为一个决策树，分裂属性就是一个树节点，分类结果是树的结点。每个节点都有左子树和右子树，结点无左右子树。

* 算法计算
    >计算公式：
### CART算法
*  算法思想
   >CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。
CART算法由以下两步组成：
   + 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；
   + 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准。

   >CART决策树的生成就是递归地构建二叉决策树的过程。
   CART决策树既可以用于分类也可以用于回归。本文我们仅讨论用于分类的CART。对分类树而言，CART用Gini系数最小化准则来进行特征选择，生成二叉树。

* 算法计算
    >计算公式：

## 决策树生成算法
* 1 向根节点输入数据
* 2 根据信息增益的度量，选择数据的某个特征来把数据分成好几例，并分别喂给一个新 Node
* 3 如果分完树后发现：
  * 1 某份数据不确定性较小，亦即某一类样本占据大多数，此时不在对数据进行划分，将对应的Node转化为叶节点
  * 2 某份数据的不确定性较大，那么这份数据就要继续分割下去

## 决策树算法流程
















